{
    "ID": 392858,
    "post_author": "909",
    "post_date": "2024-05-03 06:00:00",
    "post_date_gmt": "2024-05-03 04:00:00",
    "post_content": "<!-- wp:paragraph {\"className\":\"lecture-time-estimation\"} -->\n<p class=\"lecture-time-estimation\">Duration: approx. 30 minutes<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>In the previous text lesson, you learned about the concept of time complexity. The time complexity is the asymptotic runtime. The worst case is often taken into account, i.e. the case where the algorithm requires the most operations. The notation for the asymptotic runtime of the worst case is called <strong>big O notation<\/strong>. It is also written as follows: an algorithm could, for example, have a time complexity of <em>O(n)<\/em>.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>In this text lesson, we want to take a closer look at  big O notation. You will encounter it many times over the course of this module.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>We already said that big O notation is the asymptotic runtime in the worst case. To understand the concept, we should first understand what an asymptotic runtime is. Every algorithm has an input size <em>n<\/em>. In the asymptotic running time approach, we are interested in the growth rate of the running time <strong>in relation to the size of the input <em>n<\/em><\/strong>. We focus on the behavior of the algorithm when <em>n<\/em> approaches infinity, i.e. becomes arbitrarily large.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>Let's return to our book search algorithm to illustrate this. It can be described as follows:<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:list {\"ordered\":true,\"type\":\"1\"} -->\n<ol type=\"1\"><li>Start the search on the far left of the shelf.<\/li><li>If there is no book in the position, end the search and report that the title you are looking for was not found.<\/li><li>If there is a book, look at the title of the book. If the title matches the one you are looking for, enter the book and its position and end the search.<\/li><li>Move one position to the right and repeat all the steps from step 2.<\/li><\/ol>\n<!-- \/wp:list -->\n\n<!-- wp:paragraph -->\n<p>In our example, <em>n<\/em> is the number of books on the shelf. In the asymptotic runtime consideration, we are interested in how many additional operations we need to perform if <em>n<\/em> books are added to the shelf and the library already contains an infinite number of books. In big O notation, we are interested in the worst case. In our example, this occurs when the title is not listed on the shelf.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>How many operations do we need in this worst case scenario? We could argue that the first step is just a starting point. Then, in the worst case, we perform the following three steps <em>n<\/em> times. So we need <em>3n<\/em> operations. Does this mean that our runtime is <em>3n<\/em>? Would that be <em>O(3n)<\/em> in big O notation?<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>Not quite. We would express the time complexity of our algorithm as <em>O(n) <\/em>. Constants and factors that do not significantly influence the growth rate are virtually ignored in big O notation. This means that we are less interested in exactly how many steps happen within an iteration, as long as this number remains constant. We are mainly interested in how the total number of steps increases with increasing <em>n<\/em>. So if we have <em>3n<\/em> steps, we would refer to this as <em>O(n)<\/em> in big O notation, as the constant 3 does not change the general growth behavior. This is only relevant for big O notation if the number of steps within an iteration itself depends on <em>n<\/em>.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>The growth rate is also not significantly influenced by expressions that grow less quickly than another. For example, if two iterations run one after the other in an algorithm, both of which have different time complexities, only the dominant time complexity counts. For example, if an algorithm requires <em>n + n<sup>2<\/sup><\/em> steps in the worst case, the associated time complexity in big O notation is <em>O(n<sup>2<\/sup>)<\/em>, because n<sup>2<\/sup> grows faster than <em>n<\/em>. &nbsp;<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n[sf_box type=\"beachte\"]\n<p>Expressions that do not significantly influence the growth rate are ignored in big O notation.  <\/p>\n<ul>\n<li>Factors: the big O notation for <i>3n<\/i> steps is <i>O(n)<\/i>,<\/li>\n<li>Constants: the big O notation for <i>n+50<\/i> steps is <i>O(n)<\/i>,<\/li>\n<li>Dominated expressions: the big O notation for <i>n<sup>2<\/sup>+n<\/i> steps is <i>O(n<sup>2<\/sup>)<\/i>.<\/li>\n <\/ul>\n\n[\/sf_box]\n<!-- \/wp:html -->\n\n<!-- wp:paragraph -->\n<p>Now it's your turn! The following question describes a nonsensical variant of the book search algorithm. Your task is to determine the correct time complexity in big O notation.&nbsp;&nbsp;<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph {\"textColor\":\"black\"} -->\n<p class=\"has-black-color has-text-color\"><mark class=\"has-inline-color has-vivid-red-color\"><code>[sf_quiz id=387609]<\/code><\/mark><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n[sf_box type=\"vertiefung\"]\n<details>\n<summary>Solution explanation<\/summary>\n\nAs before, the worst case is the one where the title is not on the shelf. In each iteration step, in addition to the \"normal\" three steps (2, 3, 7), the person performing the iteration must also search the entire shelf. This creates an additional iteration within the existing iteration. In this new iteration, <i>3n<\/i> steps (4, 5, 6) are created again in each step. In total, we now count <i>n(3+3n) = 3n+3n<sup>2<\/sup><\/i> steps. However, the time complexity is not <i>O(3n<sup>2<\/sup> + 3n)<\/i> or <i>O(n+n<sup>2<\/sup>)<\/i> if we ignore factors. The last expression does not affect the growth rate from an asymptotic perspective. Therefore, the time complexity is <i>O(n<sup>2<\/sup>)<\/i>.\n\n<\/details>\n[\/sf_box]\n<!-- \/wp:html -->\n\n<!-- wp:paragraph -->\n<p><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>If you are interested in how big O is formally defined, you can read about it in the box below. <\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n[sf_box type=\"vertiefung\"]\n<details>\n<summary><strong>Deep dive:<\/strong> The formal definition of big O for time complexities.<\/summary>\n<p>\nWe can represent the number of operations required depending on the input size <i>n<\/i> as a function <i>f(n)<\/i>. If we are talking about algorithms, this function is positive at all points of <i>n<\/i>. We then look for a function <i>g(n)<\/i> that fulfills the following property: <\/p>\n<p>\nThere are constants <i>c&gt;0<\/i> and <i>k&gt;0<\/i>, so that <i>f(n) \u2264 cg(n)<\/i> applies to all instances of <i>n \u2265 k<\/i>. <\/p>\n<p>\nIf this is fulfilled, we say that the algorithm, represented by <i>f(n)<\/i>, has a time complexity of <i>O(g(n))<\/i>. We always choose the time complexity so that <i>g(n)<\/i> is as small as possible for sufficiently large <i>n<\/i>. This includes setting constants to 0 and factors to 1. <\/p>\n<p>\nThis means that the number of operations required in the asymptotic perspective (the condition <i>k \u2265 n<\/i> stands for this) never exceeds the function <i>g(n)<\/i> multiplied by a constant <i>c<\/i>, no matter how large. In our example, the time complexity is <i>O(n)<\/i>. However, the number of operations required is <i>3n<\/i>. In terms of the formal definition, it is then sufficient to show the following: <i>3n \u2264 cn<\/i> for all <i>n<\/i> that are large enough. In this example, we could choose <i>c=3<\/i> or any larger <i>c<\/i>. <\/p>\n\n\n<\/details>\n[\/sf_box]\n<!-- \/wp:html -->\n\n<!-- wp:paragraph -->\n<p><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:heading -->\n<h2>Linear running times are just about acceptable<\/h2>\n<!-- \/wp:heading -->\n\n<!-- wp:paragraph -->\n<p>We have seen above that constants, factors and dominated expressions are ignored in big O notation. This results in some typical time complexities. In ascending order the most important ones are:<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p><em>O(1)<\/em> - Constant time complexity: The best achievable time complexity is constant, written as<strong> <\/strong><em>O(1)<\/em><strong>.<\/strong> In this case, the number of steps to be executed is completely independent of the amount of data. In the formal definition, the algorithm then requires a maximum of <em>c<\/em> steps, regardless of how large the input value <em>n<\/em> is.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p><em>O(log n)<\/em> - Logarithmic time complexity: We often have to deal with this kind of time complexities in algorithms that are based on what's known as the <em>divide and conquer<\/em> principle. Here, the actual problem is repeatedly divided into similar sub-problems. If our library were sorted alphabetically in ascending order by title, we could use the following algorithm, for example:&nbsp;<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:list {\"ordered\":true,\"type\":\"1\"} -->\n<ol type=\"1\"><li>If the shelf is empty, end the search. Otherwise, start with the book in the middle of the shelf. If the number of books is even, take the book directly to the left of the center.<\/li><li>Check the title of the book. If it matches the title you are looking for, enter the book title and its position on the shelf and end the search.<\/li><li>If not, decide based on the first letter of the title you are looking for: If it is before the book title of the current book in the alphabet, continue with the left half of the shelf. If it is after it, concentrate on the right half. Start the search process in the selected area again from step 1.<\/li><\/ol>\n<!-- \/wp:list -->\n\n<!-- wp:paragraph -->\n<p>This is a classic binary search algorithm. The search set is halved at each iteration. For example, if our shelf has eight books, we would look at four books in the first iteration step, two in the second and one in the third. The maximum number of iteration steps is therefore three, whether the book is on the shelf or not. We can also express this relationship as <em>2*2*2=8<\/em> or <em>2<sup>x<\/sup>=8 <\/em>with <em>x=3<\/em>. The logarithm to the base two describes this <em>x <\/em>and therefore the number of steps required. The function is therefore nothing other than the inverse exponential function. Very important: In computer science, the logarithm usually refers to the logarithm to the base two.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p><em>O(n)<\/em> - Linear time complexity: We have already seen an example of an algorithm with linear time complexity. A time complexity like this is much less desirable than a constant or logarithmic one. In many situations, however, we need to have considered all the elements once in order to obtain the desired result. Searching an unsorted shelf is an example of this. Incidentally, we refer to this algorithm as a linear search algorithm. Other examples are finding the maximum value in a numerical list or forming the sum of all elements. We have to accept linear time complexities in many cases.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>Time complexities beyond linear are not desirable. With a quadratic time complexity, an algorithm with one thousand elements can already require one million operations, with ten thousand it needs one hundred million operations already. We rarely have to accept time complexities beyond the linear ones. Sorting a list in Python, for example, is only possible with a time complexity of <em>O(n log n)<\/em>.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>Here you can see the most important time complexities and their order:<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n<a class=\"lightbox\" href=\"\/training-content\/python-programmer\/module-05\/chapter-01\/images\/pyp_ads_t2_time_complexities.png\"><img src=\"\/training-content\/python-programmer\/module-05\/chapter-01\/images\/pyp_ads_t2_time_complexities.png\" height=\"500\"><\/a>\n<!-- \/wp:html -->\n\n<!-- wp:paragraph {\"fontSize\":\"small\"} -->\n<p class=\"has-small-font-size\"><strong>Figure 1. <\/strong>The most important time complexities.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>In this module, we will deal with the time efficiency of algorithms in great detail. To prepare, answer the following question before you start the first exercise! If you wish, you can also answer the following optional question. <\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p><mark class=\"has-inline-color has-vivid-red-color\"><code>[sf_quiz id=387615]<\/code><\/mark><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n[sf_box type=\"vertiefung\"]\n<details>\n<summary>Solution explanation<\/summary>\n<p>\nDominated expressions play no role for the growth rate. Here, sorting with <i>O(n log n)<\/i> dominates the search with <i>O(log n)<\/i>. The overall time complexity is therefore <i>O(n log n)<\/i> and therefore worse than the linear search in the unsorted shelf.   \n<\/p>\n\n\n<\/details>\n[\/sf_box]\n<!-- \/wp:html -->\n\n<!-- wp:paragraph -->\n<p><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>The following optional question refers to the formal definition of big O notation. <\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p><strong>Question (optional): <\/strong> Using the formal definition of big O notation for algorithms, show that the time complexity in the following variant of our book search algorithm is <em>O(n<sup>2<\/sup>)<\/em>. What is the smallest integer value of <em>c<\/em> that you can choose? <em>Note<\/em>: For the solution you need the Gaussian sum formula: <em>1+2+...+n = n(n+1)\/2<\/em>.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:list {\"ordered\":true,\"type\":\"1\"} -->\n<ol type=\"1\"><li>Start searching on the far left of the shelf.<\/li><li>If there is no book at the position, end the search and report that the title you are looking for was not found.<\/li><li>Otherwise, go back to the beginning of the shelf and then back to the last position you checked.<\/li><li>Move one book to the right.<\/li><li>If there is no book on the right, end the search, otherwise repeat all steps from 2).<\/li><\/ol>\n<!-- \/wp:list -->\n\n<!-- wp:html -->\n[sf_box type=\"vertiefung\"]\n<details>\n<summary>Solution<\/summary>\n<p>\nIf we count all operations individually again, three operations are required at each position. In addition to these, there are also <i>2i<\/i> operations at position <i>i<\/i> for running back and forth. The function <i>f(n)<\/i> can then be expressed by <i>f(n)=(3+2*1) + (3+2*2) + ... + (3+2*n)<\/i>. Using the Gaussian sum formula, this can be converted to <i>f(n)=4n+n<sup>2<\/sup><\/i>. Now we can use this in the formal big O definition. We want to show that <i>4n+n<sup>2<\/sup> \u2264 cn <sup>2<\/sup><\/i> holds for at least one constant <i>c<\/i>, and sufficiently large values of <i>n<\/i>. Transformation shows that the condition for <i>c=2<\/i> and <i>n \u2265 4<\/i> is fulfilled. \n<\/p>\n\n\n<\/details>\n[\/sf_box]\n<!-- \/wp:html -->\n\n<!-- wp:paragraph -->\n<p><\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>Incidentally, there are sometimes algorithms in practice that only require an unusually high number of operations in rare cases. In cases like this, big O notation is not ideal. For example, we could modify our book search algorithm so that it only has an extremely high runtime if we are looking for a specific and rarely searched title. An average analysis would then be more meaningful.<\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:html -->\n[sf_box type=\"merke\"]\n<strong>Remember:<\/strong>\n<ul>\n \t<li>Big O notation represents the upper limit of the growth of a function and is used to characterize the time complexity of algorithms.<\/li>\n \t<li>Factors, constants and dominated expressions are ignored in big O notation.&lt;\/li\n \t<li>The binary search algorithm delivers results very quickly with a time complexity of <i>O(log n)<\/i>, but requires sorted data.<\/li>\n<li>The linear search is slower in comparison with a time complexity of <i>O(n)<\/i>, but does not require sorted data.<\/li>\n<li>In special cases, where the worst case is extremely rare and has an extremely high runtime, the average analysis is more meaningful.<\/li>\n<\/ul>\n[\/sf_box]\n<!-- \/wp:html -->",
    "post_title": "Big O Notation",
    "post_excerpt": "",
    "post_status": "publish",
    "comment_status": "closed",
    "ping_status": "closed",
    "post_password": "",
    "post_name": "big-o-notation",
    "to_ping": "",
    "pinged": "",
    "post_modified": "2024-05-03 11:56:00",
    "post_modified_gmt": "2024-05-03 09:56:00",
    "post_content_filtered": "",
    "post_parent": 392855,
    "guid": "https:\/\/datalab.stackfuel.com\/?post_type=item&#038;p=392858",
    "menu_order": 22,
    "post_type": "item",
    "post_mime_type": "",
    "comment_count": "0",
    "filter": "raw",
    "meta": {
        "sf_invisibility_setting": "0",
        "_edit_lock": "1728290453:4236",
        "_edit_last": "909",
        "sf_item_type": "lecture",
        "sf_lecture_type": "text",
        "sf_training_duration": null
    }
}